# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11LVIDXylGv7G7QRXNaynSmLPeTM7WCeQ
"""

# for reading data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils

# for modeling
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping

df = pd.read_csv("PROJECT2.csv")
# shuffle the dataset! 
df = df.sample(frac=1).reset_index(drop=True)
df.head(100)

# split into X and Y
X = df.iloc[:,4:8]
Y = df.iloc[:,8:9].values
print(X.shape)
print(Y.shape)

# convert to numpy arrays
X = np.array(X)

# work with labels
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(Y)
encoded_Y = encoder.transform(Y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)

print(encoded_Y)

# build a model
model = Sequential()
model.add(Dense(4, input_shape=(X.shape[1],), activation='relu')) # input shape is (features,)
model.add(Dense(4, input_shape=(X.shape[1],), activation='relu'))
model.add(Dense(5, activation='softmax'))
model.summary()

# compile the model
model.compile(optimizer='rmsprop', 
              loss='categorical_crossentropy', # this is different instead of binary_crossentropy (for regular classification)
              metrics=['accuracy'])

import keras
from keras.callbacks import EarlyStopping

# early stopping callback
# This callback will stop the training when there is no improvement in  
# the validation loss for 10 consecutive epochs.  
es = keras.callbacks.EarlyStopping(monitor='val_loss', 
                                   mode='min',
                                   patience=10, 
                                   restore_best_weights=True) # important - otherwise you just return the last weigths...

# now we just update our model fit call
history = model.fit(X,
                    dummy_y,
                    callbacks=[es],
                    epochs=8000000, # you can set this to a big number!
                    batch_size=10,
                    shuffle=True,
                    validation_split=0.2,
                    verbose=1)

import pickle
# Saving model to disk
pickle.dump(history, open('model.pkl','wb'))

# Loadin